{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Author: LC\n",
    "# @Date:   2017-09-30 09:13:43\n",
    "# @Last Modified by:   LC\n",
    "# @Last Modified time: 2017-09-30 16:52:08\n",
    "\n",
    "###########################################################################################################\n",
    "# download images with time limit \n",
    "# bacause the method \"download_images\" in script \"download_with_selenium.py\" always block due to network issue\n",
    "# and this file is a replacement of the method \"download_images\" \n",
    "# Pay attention that time-limited strategy is to use the signal that system provides\n",
    "# and here the SIGALRM in unix-like system is adopted, so this script should run within unix-like system\n",
    "###########################################################################################################\n",
    "\n",
    "import os\n",
    "import time\n",
    "import signal\n",
    "import logging\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "from urllib.parse import urlparse\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from user_agent import generate_user_agent\n",
    "\n",
    "\n",
    "class TimeLimitError(Exception):\n",
    "    def __init__(self, value):\n",
    "        Exception.__init__()\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "\n",
    "\n",
    "def handler(signum, frame):\n",
    "    raise TimeLimitError('Time limit exceeded')\n",
    "\n",
    "\n",
    "def download_with_time_limit(link_file_path, download_dir, log_dir, limit_time = 10):\n",
    "    main_keyword = link_file_path.split('/')[-1]\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    log_file = log_dir + 'download_selenium_{0}.log'.format(main_keyword)\n",
    "    logging.basicConfig(level = logging.DEBUG, filename = log_file, filemode = \"a+\", format = \"%(asctime)-15s %(levelname)-8s  %(message)s\")\n",
    "    img_dir = download_dir + main_keyword + '/'\n",
    "    count = 0\n",
    "    headers = {}\n",
    "    if not os.path.exists(img_dir):\n",
    "        os.makedirs(img_dir)\n",
    "    signal.signal(signal.SIGALRM, handler)\n",
    "    with open(link_file_path, 'r') as rf:\n",
    "        for link in rf:\n",
    "            try:\n",
    "                ref = 'https://www.google.com'\n",
    "                o = urlparse(link)\n",
    "                ref = o.scheme + '://' + o.hostname\n",
    "                ua = generate_user_agent()\n",
    "                headers['User-Agent'] = ua\n",
    "                headers['referer'] = ref\n",
    "\n",
    "                # limit the time of downloading a image\n",
    "                try:\n",
    "                    signal.alarm(limit_time) # set a timeout(alarm)\n",
    "                    req = urllib.request.Request(link.strip(), headers = headers)\n",
    "                    response = urllib.request.urlopen(req)\n",
    "                    data = response.read()\n",
    "                except TimeLimitError as e:\n",
    "                    print('TimeLimitError: process-{0} encounters {1}'.format(main_keyword, e.value))\n",
    "                    logging.error('TimeLimitError while downloading image{0}'.format(link))\n",
    "                    continue\n",
    "                finally:\n",
    "                    signal.alarm(0) # disable the alarm\n",
    "\n",
    "                file_path = img_dir + '{0}.jpg'.format(count)\n",
    "                with open(file_path,'wb') as wf:\n",
    "                    wf.write(data)\n",
    "                print('Process-{0} download image {1}/{2}.jpg'.format(main_keyword, main_keyword, count))\n",
    "                count += 1\n",
    "                if count % 10 == 0:\n",
    "                    print('Process-{0} is sleeping'.format(main_keyword))\n",
    "                    time.sleep(5)\n",
    "            except urllib.error.HTTPError as e:\n",
    "                print('HTTPError')\n",
    "                logging.error('HTTPError while downloading image {0}http code {1}, reason:{2}'.format(link, e.code, e.reason))\n",
    "                continue\n",
    "            except urllib.error.URLError as e:\n",
    "                print('URLError')\n",
    "                logging.error('URLError while downloading image {0}reason:{1}'.format(link, e.reason))\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print('Unexpected Error')\n",
    "                logging.error('Unexpeted error while downloading image {0}error type:{1}, args:{2}'.format(link, type(e), e.args))\n",
    "                continue\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main_keywords = ['neutral', 'angry', 'surprise', 'disgust', 'fear', 'happy', 'sad']\n",
    "\n",
    "    supplemented_keywords = ['facial expression',\\\n",
    "                'human face',\\\n",
    "                'face',\\\n",
    "                'old face',\\\n",
    "                'young face',\\\n",
    "                'adult face',\\\n",
    "                'child face',\\\n",
    "                'woman face',\\\n",
    "                'man face',\\\n",
    "                'male face',\\\n",
    "                'female face',\\\n",
    "                'gentleman face',\\\n",
    "                'lady face',\\\n",
    "                'boy face',\\\n",
    "                'girl face',\\\n",
    "                'American face',\\\n",
    "                'Chinese face',\\\n",
    "                'Korean face',\\\n",
    "                'Japanese face',\\\n",
    "                'actor face',\\\n",
    "                'actress face'\\\n",
    "                'doctor face',\\\n",
    "                'movie face'\n",
    "                ]\n",
    "\n",
    "    download_dir = './data_limit_time/'\n",
    "    link_files_dir = './data/link_files/'\n",
    "    log_dir = './logs_limit_time/'\n",
    "\n",
    "    \"\"\"\n",
    "    # single process\n",
    "    for keyword in main_keywords:\n",
    "        link_file_path = link_files_dir + keyword\n",
    "        download_with_time_limit(link_file_path, download_dir)\n",
    "    \"\"\"\n",
    "    # multiple processes\n",
    "    p = Pool() # default number of process is the number of cores of your CPU, change it by yourself\n",
    "    for keyword in main_keywords:\n",
    "        p.apply_async(download_with_time_limit, args=(link_files_dir + keyword, download_dir, log_dir))\n",
    "    p.close()\n",
    "    p.join()\n",
    "    print('Finish downloading all images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "import os,shutil\n",
    "import re\n",
    "import logging\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import urllib\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from user_agent import generate_user_agent\n",
    "\n",
    "def getSearchUrl(keyWord):\n",
    "    if(isEn(keyWord)):\n",
    "        return 'https://www.google.com.hk/search?q=' + keyWord + '&safe=strict&source=lnms&tbm=isch'\n",
    "    else:\n",
    "        return 'https://www.google.com.hk/search?q=' + keyWord + '&safe=strict&hl=zh-CN&source=lnms&tbm=isch'\n",
    "\n",
    "def isEn(keyWord):\n",
    "    return all(ord(c) < 128 for c in keyWord)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def output(SEARCH_KEY_WORD):\n",
    "    global repeateNum\n",
    "    global preLen\n",
    "\n",
    "    print('搜索' + SEARCH_KEY_WORD + '图片中，请稍后...')\n",
    "\n",
    "    # 如果此处为搜搜，搜索郁金香，此处可配置为：http://pic.sogou.com/pics?query=%D3%F4%BD%F0%CF%E3&di=2&_asf=pic.sogou.com&w=05009900&sut=9420&sst0=1523883106480\n",
    "    # 爬取页面地址，该处为google图片搜索url\n",
    "    url = getSearchUrl(SEARCH_KEY_WORD);\n",
    "\n",
    "    # 如果是搜搜，此处配置为：'//div[@id=\"imgid\"]/ul/li/a/img'\n",
    "    # 目标元素的xpath，该处为google图片搜索结果内img标签所在路径\n",
    "    xpath = '//div[@id=\"rg\"]/div/div/a/img'\n",
    "\n",
    "    # 浏览器打开爬取页面\n",
    "    driver.get(url)\n",
    "\n",
    "    outputFile = OUTPUT_DIR + '/' + SEARCH_KEY_WORD + '.txt'\n",
    "    outputSet = set()\n",
    "\n",
    "    # 模拟滚动窗口以浏览下载更多图片\n",
    "    pos = 0\n",
    "    m = 0 # 图片编号\n",
    "    for i in range(PAGE_NUM):\n",
    "        if i ==8:\n",
    "            time.sleep(5)\n",
    "            elem = driver.find_element_by_xpath('//*[@id=\"smb\"]')\n",
    "            elem.click()\n",
    "            time.sleep(2)\n",
    "            print('点击加载更多')\n",
    "        pos += i*600 # 每次下滚600\n",
    "        js = \"document.documentElement.scrollTop=%d\" % pos\n",
    "        driver.execute_script(js)\n",
    "        time.sleep(1)\n",
    "        for element in driver.find_elements_by_xpath(xpath):\n",
    "            img_url = element.get_attribute('src')\n",
    "            if img_url is not None and img_url.startswith('http'):\n",
    "                outputSet.add(img_url)\n",
    "        if preLen == len(outputSet):\n",
    "            if repeateNum == 2:\n",
    "                repeateNum = 0\n",
    "                preLen = 0\n",
    "                break\n",
    "            else:\n",
    "                repeateNum = repeateNum + 1\n",
    "        else:\n",
    "            repeateNum = 0\n",
    "            preLen = len(outputSet)\n",
    "        time.sleep(3)\n",
    "        print(i)\n",
    "    print('写入' + SEARCH_KEY_WORD + '图片中，请稍后...')\n",
    "    file = open(outputFile, 'w+')\n",
    "    for val in outputSet:\n",
    "        file.write(val + '\\n')\n",
    "    file.close()\n",
    "\n",
    "    print(SEARCH_KEY_WORD+'图片搜索写入完毕')\n",
    "    print(len(outputSet))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def url_To_images(OUTPUT_DIR):\n",
    "    files = str(OUTPUT_DIR)+str(SEARCH_KEY_WORDS)+'.txt'\n",
    "    urls = open(files).read().splitlines()\n",
    "    for i in range(len(urls)):\n",
    "        try:\n",
    "            o = urlparse(urls[i])\n",
    "            ref = o.scheme + '://' + o.hostname\n",
    "            ua = generate_user_agent()\n",
    "            headers = {}\n",
    "            headers['User-Agent'] = ua\n",
    "            headers['referer'] = ref\n",
    "\n",
    "            req = urllib.request.Request(urls[i].strip(), headers=headers)\n",
    "            response = urllib.request.urlopen(req)\n",
    "            data = response.read()\n",
    "            file_path = OUTPUT_DIR + '%d.jpg'%(i)\n",
    "            with open(file_path, 'wb') as wf:\n",
    "                wf.write(data)\n",
    "            print('正在下载：', urls[i])\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(10)\n",
    "\n",
    "        time.sleep(1) #延时1秒防封IP\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 输出目录\n",
    "    OUTPUT_DIR = 'F:\\\\anacondapython\\\\谷歌爬虫\\\\Synanceia verrucosa\\\\' #保存的文件夹位置\n",
    "    # 关键字数组：将在输出目录内创建以以下关键字们命名的txt文件\n",
    "    SEARCH_KEY_WORDS = 'Synanceia verrucosa' #关键词修改这里\n",
    "    PAGE_NUM = 30 # 需要爬取的页数\n",
    "    repeateNum = 0\n",
    "    preLen = 0\n",
    "    if os.path.exists(OUTPUT_DIR) == False:\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "    # 启动Firefox浏览器\n",
    "    driver = webdriver.Firefox()\n",
    "    output(SEARCH_KEY_WORDS)\n",
    "\n",
    "    driver.close()\n",
    "    url_To_images(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
